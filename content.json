{"meta":{"title":"funny blog","subtitle":"","description":"i get angry at software","author":"Alice Vee","url":"https://kamadoori.github.io","root":"/"},"pages":[],"posts":[{"title":"Type-safe Electron IPCs","slug":"type-safe-electron-ipcs","date":"2023-11-18T19:00:00.000Z","updated":"2023-11-20T22:59:40.084Z","comments":true,"path":"2023/11/18/type-safe-electron-ipcs/","link":"","permalink":"https://kamadoori.github.io/2023/11/18/type-safe-electron-ipcs/","excerpt":"","text":"Typescript is awesome. Electron, not so much I think. Tauri is generally considered the superior option, as it’s faster, you can write code in a supposedly more enjoyable language and it provides smaller package sizes. However, Electron’s benefit is that your codebase can exist with only a single language used in it. There is a problem whenever you try to incorporate a browser window as an application with any software, and that is protection of the user’s device. With Chromium-based browsers and applications, you are sandboxing the rendering window from the rest of your OS, only allowing certain calls to send data to and retrieve data from the OS. Electron enables this with their own IPC implementation. IPC, or Inter-Process Communication, allows two separate processes to share data with one another. Electron needs to do this, as the part of the package that handles interaction with the computer (application menu, dialogs) is otherwise completely separated from the rendering part of the application (your website-turned-windows-app). The electron IPC implementation is not too difficult to wrap your head around. You have three components to it: your main file (main process), your preload file (main process) and your html&#x2F;js side (renderer process). To create an IPC channel between your main process and rendering process, you have to insert code in all three of these files. There are two ways to do so, but I will only explore the modern two-way approach of sending&#x2F;receiving data to and from the main process. First, the main.js file. This is what a cut-down version of your main.js may look like: import &#123; app, BrowserWindow, ipcMain &#125; from \"electron\"; import path from \"node:path\"; app.whenReady().then(() => &#123; ipcMain.handle(\"doThing\", async (event, args) => &#123; await doThing(); /* Do something that requires access to your OS (filesystem, registry etc) here */ return \"done\"; &#125;); const mainWindow = new BrowserWindow(&#123; webPreferences: &#123; preload: path.join(__dirname, \"preload.js\"), &#125;, &#125;); &#125;); Quite simple. You have what’s essentially an event listener waiting for a call to the doThing channel, it does a thing, then it sends back the result. By making it async we don’t have to fiddle around with callbacks quite the same way; bless modern Javascript features. Now, preload.js. This is not actually a cut-down version, your preload can be quite small: import &#123; contextBridge, ipcRenderer &#125; from \"electron\"; contextBridge.exposeInMainWorld(\"electronAPI\", &#123; doThing: () => ipcRenderer.invoke(\"doThing\"), &#125;); So what does this do? The function name exposeInMainWorld should serve as somewhat of an indication, but to be specific, this puts an electronAPI object onto the renderer process’s window object. The electronAPI object will contain all properties of the object passed as the second parameter, allowing the renderer thread to call these specific methods (and ONLY these methods, for safety). Usage of these methods becomes clear if we look at any file in the actual website that wants to use it, so let’s make a very simple index.html: &lt;!DOCTYPE html> &lt;html> &lt;head> &lt;title>Site&lt;/title> &lt;/head> &lt;body> &lt;p id=\"stuff-in-here\">Stuff!&lt;/p> &lt;button type=\"button\" id=\"btn\">Click me!&lt;/button> &lt;script defer> const p = document.getElementById(\"stuff-in-here\"); const btn = document.getElementById(\"btn\"); btn.addEventListener(\"click\", async () => &#123; const returnedValue = await window.electronAPI.doThing(); p.innerText = returnedValue; &#125;); &lt;/script> &lt;/body> &lt;/html> A simple setup; we have a button, and when you click it, doThing() is called, and the value you get back from it is put in the only paragraph element. These are the three elements there are to Electron IPC. There’s one big problem to this approach however and that is type safety. I’m creating a Nuxt 3 application, which uses Typescript for everything. I want to make sure that the types I send and retrieve are of a certain value type, and I want to be able to use that type safety in my frontend code as well as in my Electron code. How do I go about this? I have not gotten too far into this, but I have found a semi-solution that works for me. I have added a few pieces of code, some of it specifically available to Electron: // in Electron's main.ts: function addIpcMainHandler( channel: AllowedChannels, func: ElectronApiFunction ) &#123; ipcMain.handle(channel, func); &#125; // Then, in the bootstrapping process: addIpcMainHandler(\"doThing\", async () => &#123; const result = await doSomethingWithLocalFs(); return result; &#125;); // in Electron's preload.ts: function addIpcRendererHandler(api: any, channel: AllowedChannels) &#123; api[channel] = (...args: any[]) => ipcRenderer.invoke(channel, args); &#125; const api = &#123;&#125;; addIpcRendererHandler(api, \"doThing\"); contextBridge.exposeInMainWorld(\"ElectronAPI\", api); This doesn’t look too type-safe yet, does it? Here is where my shared/ folder comes in: // shared/ipc/channels.ts: export type AllowedChannels = \"doThing\" | \"doOtherThing\"; export interface ElectronApiFunction &#123; (...args: any[]): Promise&lt;any>; &#125; declare global &#123; interface Window &#123; ElectronAPI: &#123; [key in AllowedChannels]: ElectronApiFunction; &#125;; &#125; &#125; If you don’t know a lot about Typescript, this may be a bit confusing, so let’s break down what’s happening here. export interface ElectronApiFunction &#123; (...args: any[]): Promise&lt;any>; &#125; This is the signature that Electron IPC methods follow if you’re using asynchronous methods for them; anything can be a parameter, there can be any parameters, and anything can be returned from them (but in my case, it has to be a Promise). This is because I am only using Electron IPCs with two-way binding. If I used one-way binding, they’d all return void since they’re just calls to the main process to do something. This is mostly for niceness in main.ts and in any of my rendered files. It doesn’t actually do much in main.ts though, since ipcMain.handle already provides the typings for its own method. export type AllowedChannels = \"doThing\" | \"doOtherThing\"; This provides some really nice type-checking on my IPC channels though. Let’s look back at my preload.ts for a second: // no error addIpcRendererHandler(api, \"doThing\"); // Argument of type '\"foo\"' is not assignable to parameter of type 'AllowedChannels'. // vvvvv addIpcRendererHandler(api, \"foo\"); Great. I don’t have to worry about misspelling this string throughout my application anymore. If I do, my linter will just spit out an error! declare global &#123; interface Window &#123; ElectronAPI: &#123; [key in AllowedChannels]: ElectronAPIFunction; &#125;; &#125; &#125; This is really nice for your renderer process code. This piece of code essentially tells your linter “Hey, actually the window object will have an ElectronAPI property object attached to it, and this object will contain properties according to all strings in the AllowedChannels type, and these are all functions”. This means that in my renderer thread I can simply call window.ElectronAPI.doThing() and my linter will be okay with it despite me never actually having directly added anything to the window object. If I don’t add the above piece of code to my codebase, my linter will say “Property ElectronAPI does not exist on type Window &amp; typeof globalThis“. I can still call these functions though. The reason they still work is because we did put those functions on the window object, in preload.ts. Our editor just doesn’t know about that. So, what does this look like when I actually use these typed calls? Anywhere in my renderer process, I can simply call const result = await window.ElectronAPI.doThing(); …and everything just works now. One weird thing that I did run into is my default Electron configuration not working properly for setting up ipc channels. My Electron BrowserWindow object actually had contextIsolation set to false by default, which meant I could not use contextBridge.exposeInMainWorld in my application. I could also not directly set the window properties in preload.ts, which supposedly was the reason contextIsolation existed as an option in the first place. I had to re-enable it in the options to be able to expose the functions to the window object, but then it worked fine. For now, that’s as far as I have gotten with type safety in Electron. What I want to work on more is being able to define the correct signature for the functions I’m calling and have those return types show up in my editor as well (if window.ElectronAPI.doThing() is supposed to return a string, then I want vscode to give me an error if I don’t treat the return value as one). That’s something left for next time though. Now I have to deep dive into CSS hell to figure out how I am going to style this application I’m working on.","categories":[],"tags":[]},{"title":"Screen Shaders in Godot","slug":"screen-shaders-in-godot","date":"2023-07-18T12:00:00.000Z","updated":"2023-11-20T22:59:40.084Z","comments":true,"path":"2023/07/18/screen-shaders-in-godot/","link":"","permalink":"https://kamadoori.github.io/2023/07/18/screen-shaders-in-godot/","excerpt":"","text":"There are some premade options out there for PSX shaders in Godot. They allow you to apply materials to objects and apply shaders to the screen to emulate the rendering of the PSX. The premade options were unfortunately either somewhat lacking or had to be converted to be compatible with Godot 4. Trying to convert them from Godot Shader Language 1 to 2 was dramatic and plagued me with issues, so instead I decided to learn how to write these shaders from scratch. That was a very interesting process to go through. Getting StartedFirst off, I set up a sample project. No skybox, simple texture on floor and ability to move around. Nearly immediately I run into a problem; Godot’s post-processing stack is kinda nasty. To add custom post processing, you have to have a node stack like this: CanvasLayer └── SubViewportContainer &lt;-- Contains Screen space shader └── SubViewport └── Camera This doesn’t seem so bad, right? There are some problems with that though. Since your camera has to be under the viewport, you probably need your entire game under there. It’s manageable enough, although needs some finaggling when you’re loading other scenes, unless you are going to swap the entire scene out for another or pack your cameras into complete scenes with the CanvasLayers and SubViewports. Additionally, If you want to add another pass, you need another two nodes. The nodes are also in an initially illogical order. CanvasLayer └── SubViewportContainer &lt;-- Contains Second Pass └── SubViewport └── SubViewportContainer &lt;-- Contains First Pass └── SubViewport └── Camera The order makes some sense if you think about the viewport containers as translucent sheets of plastic; you have your camera screen, then you put your first pass on it, then you put your second pass on that. Okay, so I have my stack set up correctly. Time for a completely innocuous shader, one pass, to test everything. shader_type canvas_item; uniform sampler2D screen_texture: hint_screen_texture, filter_nearest, repeat_disable; void fragment() &#123; vec4 c = texture(screen_texture, SCREEN_UV); COLOR = c; &#125; Aaand… …nothing. We browse the Godot shader reference. Surely there’s something I’m missing. From https://docs.godotengine.org/en/stable/tutorials/shaders/custom_postprocessing.html : shader_type canvas_item; uniform sampler2D screen_texture : hint_screen_texture, repeat_disable, filter_nearest; // Blurs the screen in the X-direction. void fragment() &#123; vec3 col = texture(screen_texture, SCREEN_UV).xyz * 0.16; // [snipped some blur calculations here] COLOR.xyz = col; &#125; Okay.. I didn’t miss anything here I think. Maybe there’s just something about the alpha? Let’s try this. shader_type canvas_item; uniform sampler2D screen_texture : hint_screen_texture, repeat_disable, filter_nearest; void fragment() &#123; vec3 c = texture(screen_texture, SCREEN_UV).xyz; COLOR.rgb = c.rgb; &#125; Nope, still a gray screen. After bashing rocks together for a little bit longer, I found that actually, screen_texture is not the right variable to use. Instead I found the following at https://docs.godotengine.org/en/stable/tutorials/shaders/shader_reference/canvas_item_shader.html : Built-In Description sampler2D TEXTURE Default 2D texture. So what do you think this contains? What does “default 2D texture” mean? Surely it’s not the screen texture, that should be in screen_texture, right? Nope. This is the right variable. shader_type canvas_item; void fragment() &#123; vec3 c = texture(TEXTURE, SCREEN_UV).rgb; COLOR = c.rgb; &#125; Hooray, output. I wasn’t too happy at this point to be completely honest. The fact that the first sample on the wiki doesn’t work is very annoying, and something I alluded to when talking about Unity previously. At least I can get started on writing screen shaders now. Just a quick check that I’m not totally crazy: void fragment() &#123; vec4 c = texture(TEXTURE, SCREEN_UV); COLOR = vec4(FRAGCOORD.x * SCREEN_PIXEL_SIZE.x, FRAGCOORD.y * SCREEN_PIXEL_SIZE.y, 255, c.a); &#125; How lovely. How about some quantization? const float STEPS = 8.0; void fragment() &#123; vec2 approx = floor(FRAGCOORD.xy * SCREEN_PIXEL_SIZE * vec2(STEPS)) / vec2(STEPS); COLOR.rgb = vec3(approx, 255.0); &#125; Now that this is working, let’s finally try applying an effect to the actual screen texture. const float STEP = 8.0; void fragment() &#123; vec4 c = texture(TEXTURE, SCREEN_UV); vec4 rounded = floor(c * STEP) / STEP; COLOR = rounded; &#125; All good. Now I can work on creating my own shaders. I’ll have to look into how to apply dithering to a picture via screen space shaders, as that’s what the original PS1 did.","categories":[],"tags":[]},{"title":"Unity Struggles","slug":"unity-struggles","date":"2023-07-13T02:00:00.000Z","updated":"2023-11-20T22:59:40.084Z","comments":true,"path":"2023/07/13/unity-struggles/","link":"","permalink":"https://kamadoori.github.io/2023/07/13/unity-struggles/","excerpt":"","text":"I’m really starting to hate Unity. I have been using Unity on-and-off since somewhere around 2017.4 . The interesting thing to note is that I actually didn’t hate Unity all that much back in 2017. It didn’t have a lot of features I take for granted now (especially 2D, the 2D packages are actually quite nice) but I really didn’t miss those features. I could not be bothered to learn how to work with the Godot scripting language or node system in any capacity, so that was also in favor of Unity. Pretty recently I’m starting to feel like Godot is actually way more competent in many aspects. Getting StartedWant to get started in Godot? Download a zip, unzip it, run the executable. Create new project, wait 3 seconds after clicking Create, You’re completely done. Want a headstart? Download one of the example projects. No external tooling needed. You can use external tooling like VSCode to edit code, but I found that Godot’s language server in Godot 4 was kinda slow, so I just edit everything in-editor. Want to get started in Unity? Download the Unity Hub. Log into your Unity account. Register if you do not have one yet. Accept the license agreements. Accept the popup confirming you’re using a personal license. DO NOT let the hub install the version it’s about to install because it will most likely be a version that sucks. Instead download the LTS. If the LTS is the latest version, download the previous LTS. Either cope by using Visual Studio, cope by using Visual Studio Code’s stinkfest of a Unity setup or pay a monthly fee to your good friends over at JetBrains to use Rider. Pray that Unity does not break your code editor’s package setup this time around; I’ve found that the VSCode and Rider packages are constantly fighting with each other over stupid things. Wait upwards of 10 minutes for your project to finish being made, especially if you use the URP Core packages. Don’t expect the “micro projects” Unity advertised to show up or even work. Iteration SpeedGodot: Click play, have the game start up at your selected scene, go for it. Extremely fast for smaller projects. No compilation required since it’s all scripted. Unity: Has a really strong chance to have the upper hand here as the game is “loaded” when the engine’s up. Problem is that Unity’s domain compilation and reloading is so stupidly slow. Click play, wait five seconds in a completely fresh project, you can start testing. Add another five seconds to the iteration time if it’s right after you edit a script. And yes, you can disable domain reload on play. Problem is that this can introduce bugs that you might not be expecting. Although Unity already is pretty good at that to the point where power users suggest you just close and open the project every couple hours to refresh things and keep Unity running “fast”. Problem with that is that this can easily add another minute of waiting! Now why are the times I mentioned here important? Surely a couple of seconds to power on the game and get going isn’t a problem, is it? The problem is that when you constantly have to wait for things, you start doing other things. You don’t sit behind your PC waiting for a compilation to happen unless you just stood up for something. You grab a drink, grab a snack, go to the toilet, maybe browse the social media tab you have open in the background. And once in a while you’ll randomly forget that you were actually working on a Unity project. I don’t have this problem with Godot. I keep my focus because Godot isn’t trying to ruin my focus. Input SystemGodot’s input system is wonderful. Add action, add binding, listen for binding, done. You can mix and match event listening and polling. class_name Player var speed = 5.0 var jump_force = 4.5 func _input(event): if event.is_action_pressed(\"jump\"): jump() func _physics_process(delta): if Input.is_action_pressed(\"move_right\"): position.x += speed * delta move_and_slide() # Applies velocity and all that collision goodness for you. func _jump(): velocity.y = 4.5 Unity’s Input Manager is clunky in comparison. You’re essentially polling by default; it’s not reactive. public class Player : MonoBehaviour &#123; float speed = 5f; void Update() &#123; if(Input.GetButtonDown(\"jump\")) &#123; Jump(); &#125; if(Input.GetButtonDown(\"move_right\")) &#123; transform.Translate(new Vector3(speed * Time.deltaTime, 0, 0)); &#125; &#125; void Jump() &#123; // There's no built-in velocity unless you use Rigidbodies by the way. // So add a rigidbody component, restrict its rotation on the X and Z // axis to prevent your character from toppling over, and add a // reference to the rigidbody on this script. &#125; &#125; Unity’s Input System is even worse. Even though it seems pretty powerful and you can mix-and-match events, setting it up is a drag: public class Player : MonoBehaviour &#123; float speed = 5f; [SerializeField] InputAction moveAction; // Don't forget to assign this! void OnActionChange(object obj, InputActionChange change) &#123; if ( ((InputAction) obj).name == \"Jump\" &amp;&amp; change == InputActionChange.ActionStarted ) &#123; Jump(); &#125; &#125; void OnEnable() &#123; InputSystem.onActionChange += OnActionChange; moveAction.Enable(); &#125; void OnDisable() &#123; InputSystem.onActionChange -= OnActionChange; moveAction.Disable(); &#125; void Update() &#123; var value = moveAction.ReadValue&lt;Vector2>(); transform.Translate(new Vector3(value * speed * Time.deltaTime)); &#125; void Jump() &#123; // Still no built in velocity. &#125; &#125; On the note of the new Input System, it’s a drag to install and most packages won’t work with it! So when you install the new Input System, you’ll need to have both the Input System and Input Manager active in the background. When you install the System that’s another engine restart by the way, because Unity needs to enable the backend for it or something. In-engine modelingGodot has CSG nodes built-in. You can do boolean operations with each shape and rather quickly build up a small platform to work with. Although it’s nice to work with, I cannot call it ideal; you still want to do your actual worldbuilding outside of Godot. Godot does have terrain tools, but like Unity’s I feel like they’re.. just kinda usable. Just like how Cities: Skylines’s terrain tool is usable. Speaking of Unity; Probuilder. I don’t like it. It constantly freaks out and it annoys me. CommunityI feel like Unity makes up a lot of lost space in this regard. The amount of content available for Unity is outstanding. It’s just a shame that nearly all of it is outdated in one way or another. The Godot community is also.. weirdly elitist at times. I don’t know if it’s just me accidentally happening upon those “Well if you Googled this..” folks or not, but it’s kinda frustrating to see, especially for software that could really use an increase in community participation. You’ve got these people willing to do things in your community, just help them. It would’ve taken you as much time to solve some of these issues as it would’ve to google the issue yourself, check that the issue you googled is actually answered in the link you’re about to send and sending the link. DocumentationNeither documentation is ever properly up-to-date. It’s easy to pick a reason and stick it to me, but it’s also a massive issue for developers. If Godot breaks their shader code compatibility, I want an upgrade path. If Unity breaks their shader code compatibility, I want an upgrade path. I want to know what I can do in these engines. I don’t want to happen upon a function that is not explained in the docs but is explained in a blog post from 2014 made by someone who knows three times as much about the engine as core developers seem to do. So why use Unity?Assets. Godot is lacking in code assets pretty severely, especially with a major version bump earlier this year. Unity’s Asset Store has a lot of really good code assets. Art assets (textures, sprites, animations, music etc.) aren’t actually that big of a deal since: They are (usually) compatible between the two assets You can get them on so many different websites it’s actually laughable One interesting one for me is trying to develop a game with PSX style limitations to it. Affine texture mapping, vertex warping, dithering, color depth reduction, all that good stuff. One massive issue I run into with both engines is the complete lack of support for hard edge shadows which seems so extremely silly to me. Unity of course has a solution in the Asset store and PFFFTTTT- Yeah. I’m not spending that. It also only works with the Universal Rendering Pipeline. There aren’t too many PSX games with stencil shadows but it’s still an effect I absolutely adore. I guess using a shadow circle around the character would have to do. I think it can be done rather easily: cast a light on a separate layer on a mesh that’s also on that layer, make the mesh invisible but the shadow cast not. I think that’s feasible at least. So that kinda leaves me inbetween a rock and a hard place. I’m not adept with shaders, and learning to be would take a long time. Especially considering the difference in dialect between Godot’s shading language and Unity’s, and how both of these relate to GLSL&#x2F;HLSL. Just working with Unity puts me in a sour mood, but at least I get to watch Interstellar today. rev. 2023&#x2F;07&#x2F;13: spelling fix","categories":[],"tags":[]},{"title":"zzz","slug":"hello-world","date":"2023-07-11T22:00:00.000Z","updated":"2023-11-20T22:59:40.084Z","comments":true,"path":"2023/07/11/hello-world/","link":"","permalink":"https://kamadoori.github.io/2023/07/11/hello-world/","excerpt":"","text":"got angry at software and needed an outlet for all my problems","categories":[],"tags":[]}],"categories":[],"tags":[]}